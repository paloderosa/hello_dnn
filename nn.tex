\documentclass[aps,10pt]{revtex4}
\usepackage[paperwidth=210mm,paperheight=297mm,centering,hmargin=2cm,vmargin=2.5cm]{geometry}
%\documentclass[letter,11pt]{article}
%\usepackage[margin=2.5cm]{geometry}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{algpseudocode}

\DeclareGraphicsExtensions{.eps,.ps,.pdf}
\DeclareGraphicsRule{*}{mps}{*}{}

%\renewcommand{\baselinestretch}{1.5}
\newcommand{\e}{\text{e}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem*{note}{Note}


\begin{document}

\title{Notes on forward and backward propagation in deep neural networks}
\date{\today}
\maketitle
%\tableofcontents

\vspace{2cm}

\noindent The goals of these notes are twofold:
\begin{itemize}
	\item expressing mathematically the procedures of forward and backward propagation;
	\item getting familiar with the notation employed in a full vectorized form of these operations, over features, hidden units, output units and training examples.
\end{itemize}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{nn_diagram}
	\caption{Architecture for a neural network with three layers: one input layer, 2 hidden layers and one output layer.}
	\label{fig:nndiagram}
\end{figure}
To the formality oriented, as myself, it is evident that forward and backward propagation are some complicated forms of function composition and chain rule application, respectively. However, these are not arbitrary instances of these mathematical rules. There is an underlying diagrammatic representation that defines the neural network and which we should take advantage of, but also that limits the class of functions expressible as the result of forward propagation. The generic architecture of a deep neural network is shown in Figure \ref{fig:nndiagram}. We count the total number of layers including the output layer but excluding the input layer. Therefore a neural network with $L$ layers consists of one input layer, followed by $L-1$ hidden layers, followed by an output layer.

\section{Forward propagation}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{nn_diagram_2_layers}
	\caption{Architecture for a shallow neural network with one hidden layer. The specifics of this network suffice to formulate forward and backward propagation for any depth neural network.}
	\label{fig:nndiagram2layers}
\end{figure}


\noindent Let us consider the simplest case of a deep neural network, that with a single hidden layer and a single output unit, appropiate for a binary classification problem (see Figure \ref{fig:nndiagram2layers}). At the moment, we are going to consider only one training example $(\mathbf{x},y)$, with $\mathbf{x}$ arranged as a column vector with $n^{[0]}$ entries and $y$ a single classification output, as
\begin{equation}
 \mathbf{x} = 
 \begin{pmatrix}
 x_1 \\
 \vdots \\
 x_n
 \end{pmatrix}\in\mathbb{R}^{n^{[0]}}, \qquad y\in\{0,1\}.
\end{equation}
The first and only hidden layer has $n^{[1]}$ units, which are activated by a function $\chi^{[1]}$ (tanh, relu or sigmoid, for example) and the output unit is activated by a function $\chi^{[2]}$. The diagrammatic picture is the following. The circles in the input layer, also called the input units, the first on the left, represent each a feature $a^{[0]}_i = x_i$, so we have in total $n$ units. The following column of circles represent the units in the hidden layer. Consider a single unit $j$ in the hidden layer. This receives as input the units from the preceding layer and outputs a real function $a^{[1]}_j$, so that
\begin{equation}
a^{[1]}_j : \mathbb{R}^{n^{[0]}}\rightarrow\mathbb{R},
\end{equation}
which is produced by first computing
\begin{equation}
 z_j^{[1]} = \sum_k W_{jk}^{[1]}a^{[0]}_k + b_j^{[1]},
\end{equation}
followed by the activation function $\chi^{[i]}$, so that
\begin{equation}
a_j^{[1]} = \chi^{[1]}\left(\sum_k W_{jk}^{[1]}a^{[0]}_k + b_j^{[1]}\right),
\end{equation}
where $W$ is a $n^{[1]}\times n^{[0]}$ matrix and $b^{[1]}$ is a $n^{[1]}\times 1$ matrix. And this happens for each unit in the hidden layer. Finally, the output unit performs the same action, takes as input the activation units from its preceding layer and outputs
\begin{equation}
 a^{[2]} = \chi^{[2]}\left(\sum_k W_{1k}^{[2]}a^{[1]}_k + b^{[2]}\right),
\end{equation}
where now $W^{[2]}$ is a $1\times n^{[1]}$ matrix and $b^{[2]}$ is a $1\times 1$ matrix. The output $a^{[2]}$ can be seen as $a^{[2]} = a^{[2]}\left(\mathbf{x};W^{[1]},W^{[2]},b^{[1]},b^{[2]}\right)$. This can be considered as a function $\mathbb{R}^{n^{[0]}}\rightarrow\mathbb{R}$ with parameters $W^{[1]}$, $W^{[2]}$, $b^{[1]}$, $b^{[2]}$ when, which varied, generate an entire subclass of functions.
\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{nn_single_unit}
	\caption{Forward propagation over a single layer.}
	\label{fig:nnsingleunit}
\end{figure}

\section{Optimization task}
\noindent Let us define the loss function
\begin{equation}
\mathcal{L}(a^{[2]},y) = -y\log a^{[2]} - (1-y)\log\left(1-a^{[2]}\right).
\end{equation}
This function penalizes that $a^{[2]}$ differs from $y$. We fix $\mathbf{x}$ and consider the optimization task of finding the parameters $W^{[1]}$, $W^{[2]}$, $b^{[1]}$, $b^{[2]}$ that minimize the loss function. This is achieved by starting from a random point in the space of parameters and following the path of greatest descent until some minimum is found. Since the surface over parameter space corresponding to the loss function is not convex, this minimum might be a local minimum. Random initialization allows to arrive at the global minimum. The path of greatest descent parameterized by $s$ is defined by
\begin{equation}
 \frac{d\theta_i}{ds} = - \sum_j \delta_{ij}\frac{\partial\mathcal{L}}{\partial\theta_j},
\end{equation}
where $\theta_i$ is some parameter and $\delta_{ij}$ is the metric in these coordinates. Therefore, it is important that we obtain an expression for the derivates of $\mathcal{L}$ with respect to parameters.

\section{Backward propagation}
\noindent Let us first consider the derivatives of $\mathcal{L}$ with respect to parameters in the second layer, $W^{[2]}$, $b^{[2]}$. Typically the output unit is activated with the sigmoid function $\sigma(z)$, which satisfies $\sigma'(z) = \sigma(z)(1-\sigma(z))$. We have then
\begin{equation}
 \begin{aligned}
 	\frac{\partial\mathcal{L}}{\partial z^{[2]}} & = \frac{\partial\mathcal{L}}{\partial a^{[2]}}\frac{d a^{[2]}}{d z^{[2]}} = a^{[2]}-y,\\
	\frac{\partial\mathcal{L}}{\partial W_{1k}^{[2]}} & = \frac{\partial\mathcal{L}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial W_{1k}^{[2]}} = \left(a^{[2]}-y\right)a^{[1]}_k, \\
	\frac{\partial\mathcal{L}}{\partial b^{[2]}} & = \frac{\partial\mathcal{L}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial b^{[2]}} = a^{[2]}-y.
 \end{aligned}
\end{equation}
That was easy. Let us now compute the derivatives of the loss function with respect to farther parameters, those in the first layer, $W^{[1]}$, $b^{[1]}$. In this case,
\begin{equation}
 \begin{aligned}
 \frac{\partial\mathcal{L}}{\partial z^{[1]}_j} & = \sum_l\frac{\partial\mathcal{L}}{\partial a^{[2]}}\frac{d a^{[2]}}{d z^{[2]}}\frac{\partial z^{[2]}}{\partial a^{[1]}_l}\frac{d a^{[1]}_l}{d z^{[1]}_j} = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j},\\ 
  \frac{\partial\mathcal{L}}{\partial W^{[1]}_{jk}} & = \sum_{m}\frac{\partial\mathcal{L}}{\partial z^{[1]}_m}\frac{\partial z^{[1]}_m}{\partial W_{jk}^{[1]}}  = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j}a_k^{[0]}, \\
  \frac{\partial\mathcal{L}}{\partial b^{[1]}_{j}} & = \sum_{m}\frac{\partial\mathcal{L}}{\partial z^{[1]}_m}\frac{\partial z^{[1]}_m}{\partial b_{j}^{[1]}}  = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j}.
 \end{aligned}
\end{equation}

In order to formulate a vectorization over features and units of the previous expressions, we consider the following. First, the series of derivatives along a set of parameters will be contained in an object of the same form as those parameters. For example, since $W^{[2]}$ is a $1\times n^{[1]}]$ matrix, then $\nabla_{W^{[2]}}$ should be of the same form. Additionally, we define an operation $*$ ocurring between matrices of the same and denoting element-wise multiplication. Then a vectorization is given by
\begin{equation}\label{eqn:variation2}
 \begin{aligned}
  \nabla_{z^{[2]}}\mathcal{L} & = a^{[2]}-y, \\
  \nabla_{W^{[2]}}\mathcal{L} & = \left(\nabla_{z^{[2]}\mathcal{L}}\right)\left(a^{[1]}\right)^T, \\
  \nabla_{b^{[2]}}\mathcal{L} & = \left(\nabla_{z^{[2]}}\mathcal{L}\right),
 \end{aligned}
\end{equation}
as well as
\begin{equation}\label{eqn:variation1}
\begin{aligned}
\nabla_{z^{[1]}}\mathcal{L} &  = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz},\\ 
\nabla_{W^{[1]}}\mathcal{L} & = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz}\left(a^{[0]}\right)^T, \\
\nabla_{b^{[1]}}\mathcal{L} & = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz}.
\end{aligned}
\end{equation}

\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{nn_2_layers_backprop}
	\caption{}
	\label{fig:nn2layersbackprop}
\end{figure}

Let us express the previous results from a diagrammatic perspective, considering Figure \ref{fig:nn2layersbackprop}. Let us sit at the output unit, where the variation in the loss due to a variation in the linear output $z^{[2]}$ is $a^{[2]} - y$. Then we append that value to that unit's output. Then, going backward, we consider the line that joins the output unit with the $k$-th unit from the hidden layer. This line contributes with the weight $W^{[2]}_k$, whose variation produces a variation in the loss given by (\ref{eqn:variation2}). Then we only multiply the value from the left with the activation $a^{[1]}_k$, the same as for the bias unit activation (which equals one). As we go on, from the $k$-th unit in the hidden layer to the $j$-th unit in the input layer, we multiply first by $W^{[2]}_{ik}d\chi^{[1]}(z_k)/dz$ and second by the activation $a_{[j]}$ or by one for the bias unit, according to (\ref{eqn:variation1}).

\section{Vectorizing over $m$ examples}
\noindent We now include in the $n^{[0]}\times m$ matrix $X$ all the training examples with each training $\mathbf{x}^a$ example as column vector. We also consider $n^{[2]}\times m$ vector $Y$ as consisting of all the training results. Then we have
\begin{equation}
X = \begin{pmatrix}
 x^1_1 & x^2_1 & \cdots & x^{m-1}_1 & x^m_1 \\
 x^1_2 & x^2_2 & \cdots & x^{m-1}_2 & x^m_2 \\
 \vdots & \vdots & \ddots & \vdots & \vdots \\
 x^1_{n^{[0]}-1} & x^2_{n^{[0]}-1} & \cdots & x^{m-1}_{n^{[0]}-1} & x^m_{n^{[0]}-1} \\
 x^1_{n^{[0]}} & x^2_{n^{[0]}} & \cdots & x^{m-1}_{n^{[0]}} & x^m_{n^{[0]}} \\
\end{pmatrix}, \qquad 
Y = \begin{pmatrix}
y^1 & y^2 & \cdots & y^{m-1} & y^m \\
\end{pmatrix}.
\end{equation}
We consider $A^{[0]} = X$. Then \textbf{forward propagation} consists in the following series of steps
\begin{enumerate}
	\item linear propagation to the first layer (yielding a $(n^{[1]}\times n^{[0]})\cdot(n^{[0]}\times m) = n^{[1]}\times m$ matrix):
	\begin{equation}
	Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]};
	\end{equation}
	\item unit activation in the first layer:
	\begin{equation}
	 A^{[1]} = \chi^{[1]}\left(Z^{[1]}\right);
	\end{equation}
	\item linear propagation to the second layer (yielding a $(n^{[2]}\times n^{[1]})\cdot(n^{[1]}\times m) = n^{[2]}\times m$ matrix):
	\begin{equation}
	Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]};
	\end{equation}
	\item unit activation in the second layer:
	\begin{equation}
	A^{[2]} = \sigma\left(Z^{[2]}\right).
	\end{equation}
\end{enumerate}
We define $\dot{1}_{n_1\times n_2}$  as a $n_1\times n_2$ matrix full of 1's, which can be programmed through broadcasting operations. We compute the \textbf{cost function} as the average of the individial losses over all examples as
\begin{equation}
 \mathcal{L}\left(A^{[2]},Y\right) = -\frac{1}{m}\left[\left(\log A^{[2]}\right) Y^T + \left(\dot{1}_{n^{[2]}\times m}-\log A^{[2]}\right) \left(\dot{1}_{n^{[2]}\times m} - Y\right)^T\right].
\end{equation}
Finally, \textbf{backward propagation} is performed through the following series of steps:
\begin{enumerate}
	\item output layer linear variation:
	\begin{equation}
	 \nabla_{Z^{[2]}} \mathcal{L} = \frac{1}{m}\left(A^{[2]} - Y\right);
	\end{equation}
	\item weights and biases variations in the second layer:
	\begin{eqnarray}
	 \nabla_{W^{[2]}}\mathcal{L} & = \nabla_{Z^{[2]}} \mathcal{L} \cdot \left(A^{[1]}\right)^T, \\
	 \nabla_{b^{[2]}}\mathcal{L} & = \nabla_{Z^{[2]}} \mathcal{L} \cdot \left(\dot{1}_{1\times m}\right)^T;
	\end{eqnarray}
	\item hidden layer linear variation:
	\begin{equation}
	\nabla_{Z^{[1]}} \mathcal{L} = \left[\left(W^{[2]}\right)^T\cdot\nabla_{Z^{[2]}} \mathcal{L}\right]*\frac{d\chi^{[1]}(Z^{[1]})}{dz}
	\end{equation}
	\item weights and biases variations in the second layer:
	\begin{eqnarray}
	\nabla_{W^{[1]}}\mathcal{L} & = \nabla_{Z^{[1]}} \mathcal{L} \cdot \left(A^{[0]}\right)^T, \\
	\nabla_{b^{[1]}}\mathcal{L} & = \nabla_{Z^{[1]}} \mathcal{L} \cdot \left(\dot{1}_{1\times m}\right)^T;
	\end{eqnarray}
\end{enumerate}

\section{Generalizing to arbitrary number of hidden layers}
\noindent From the preceding discussion, it is straightforward to derive a generalization to a neural network with an arbitrary number of layers. Let us consider a neural network with the following architecture:
\begin{figure}[h]
	\centering
	\includegraphics[width=0.9\linewidth]{deep_network_architecture}
	\caption{}
	\label{fig:deepnetworkarchitecture}
\end{figure}

\noindent\textbf{Forward propagation}

\noindent\fbox{%
	\parbox{\textwidth}{%		
	\begin{algorithmic}
		\State {$A^{[0]} \gets X$} 
		\For {$l = 1,\ldots, L$}
		\State {$Z^{[l]} \gets W^{[l]} A^{[l-1]} + b^{[l]}$}
		\State {$A^{[l]} \gets \chi^{[l]}\left(Z^{[l]}\right)$}
		\EndFor
	\end{algorithmic}
	}%
}

\vspace{1cm}
\noindent\textbf{Backward propagation}

\noindent\fbox{%
	\parbox{\textwidth}{%		
		\begin{algorithmic}
			\State {$\nabla_{Z^{[L]}}\mathcal{L} \gets \frac{1}{m}\left(A^{[L]} - Y\right)$} 
			\For {$l = L,\ldots, 1$}
			\State {$\nabla_{W^{[l]}}\mathcal{L} \gets \nabla_{Z^{[l]}} \mathcal{L} \cdot \left(A^{[l-1]}\right)^T$}
			\State {$\nabla_{b^{[l]}}\mathcal{L} \gets \nabla_{Z^{[l]}} \mathcal{L} \cdot \left(\dot{1}_{n^{[l]}\times m}\right)^T$}
			\State {$\nabla_{Z^{[l-1]}} \mathcal{L} \gets \left[\left(W^{[l]}\right)^T\cdot\nabla_{Z^{[l]}} \mathcal{L}\right]*\frac{d\chi^{[l-1]}(Z^{[l-1]})}{dz}$}
			\EndFor
		\end{algorithmic}
	}%
}


%\begin{thebibliography}{11}
% \bibitem{Wald:1984}
%  Robert Wald, {\em General Relativity}. The University of Chicago Press, 1984.
%\end{thebibliography}

\end{document}