import numpy as np

def sigmoid(Z):
    """
    sigmoid function applied element-wise to Z
    :param Z: array
    :return: sigmoid(array)
    """
    return 1/(1 + np.exp(-Z))

def tanh(Z):
    """
    tanh function applied element-wise to Z
    :param Z: array
    :return: tanh(array)
    """
    return np.tanh(Z)

def dtanh(Z):
    """
    derivative of tanh applied element-wise to Z
    :param Z: array
    :return: dtanh(array)
    """
    return 1 - np.square(tanh(Z))


def initialize_parameters(layer_dims, type = "random"):
    """
    initializes parameters W1, b1, ..., WL, bL
    :param layer_dims: list of layer dimensions (input, hidden, output)
    :param type: type of initialization
    :return: initialized parameters
    """
    depth = len(layer_dims)
    parameters = {}

    for l in range(1,depth):
        parameters["W" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01
        parameters["b" + str(l)] = np.zeros((layer_dims[l],1))

    return parameters


def forward_propagation(X, parameters):
    """
    computes the output function generated by the neural network, as well as the intermediate linear transforms and
    activations Z1, A1, ..., ZL, AL
    :param X: training data x1, ...,xm
    :param parameters: parameters W1, b1, ..., WL,bL
    :return: AL, cache of linear transforms and activations
    """
    depth = len(parameters)//2

    cache = {}
    A_temp = np.copy(X)

    # propagation for the hidden layers with tanh activation
    for l in range(1, depth):
        W_temp = parameters['W' + str(l)]
        b_temp = parameters['b' + str(l)]

        Z_temp = np.dot(W_temp, A_temp) + b_temp
        A_temp = tanh(Z_temp)

        cache['Z' + str(l)] = Z_temp
        cache['A' + str(l)] = A_temp

    # propagation for the output layer with sigmoid activation
    W_temp = parameters['W' + str(depth)]
    b_temp = parameters['b' + str(depth)]

    Z_temp = np.dot(W_temp, A_temp) + b_temp
    A_temp = sigmoid(Z_temp)

    cache['Z' + str(depth)] = Z_temp
    cache['A' + str(depth)] = A_temp

    return A_temp, cache


def cost_function(AL, Y):
    """
    cross-entropy cost function
    :param AL: neural network output
    :param Y: training outputs
    :return: cross-entropy cost function
    """
    m = AL.shape[1]
    return -(1./m) * (np.dot(np.log(AL),Y.T) + np.dot(1-np.log(AL),1-Y.T))


def backward_propagation(cache, parameters, AL, X, Y):
    """
    performs backward propagation to compute derivatives of parameters W1, b1, ..., WL, bL
    :param cache: cache of linear transforms and activations Z1, A1, ..., ZL, AL
    :param parameters: parameters W1, b1, ..., WL, bL
    :param AL: neural network output
    :param X: training inputs
    :param Y: training outputs
    :return: derivatives of parameters
    """
    m = AL.shape[1]
    depth = len(parameters)//2

    dZ_temp = (1./m) * (AL - Y)
    grads = {}

    for l in reversed(range(2,depth+1)):
        Z_temp = cache['Z' + str(l - 1)]
        A_temp = cache['A' + str(l - 1)]

        W_temp = parameters['W' + str(l)]

        grads['dW' + str(l)] = np.dot(dZ_temp,A_temp.T)
        grads['db' + str(l)] = np.sum(dZ_temp, axis = 1, keepdims=True)

        dZ_temp = np.dot(W_temp.T, dZ_temp) * dtanh(Z_temp)

    grads['dW' + str(1)] = np.dot(dZ_temp, X.T)
    grads['db' + str(1)] = np.sum(dZ_temp, axis = 1, keepdims=True)

    # print(grads.keys())

    return grads


def update_parameters(cache, parameters, AL, X, Y, learning_rate):
    """
    performs batch gradient descent
    :param cache: cache of linear transforms and activations Z1, A1, ..., ZL, AL
    :param parameters: parameters W1, b1, ..., WL, bL
    :param AL: neural network output
    :param X: training inputs
    :param Y: training outputs
    :param learning_rate:
    :return: updated parameters after one iteration of batch gradient descent
    """
    depth = len(parameters)//2
    grads = backward_propagation(cache, parameters, AL, X, Y)

    for l in range(1,depth + 1):
        parameters['W' + str(l)] = parameters['W' + str(l)] - learning_rate * grads['dW' + str(l)]
        parameters['b' + str(l)] = parameters['b' + str(l)] - learning_rate * grads['db' + str(l)]

    return parameters


def neural_network_model(X, Y, layer_dims, iterations = 500, learning_rate = 0.01):
    """
    neural network model with arbitrary hidden layers
    :param X: training inputs
    :param Y: training outputs
    :param layer_dims: list of layer dimensions (input, hidden, output)
    :param iterations: number of iterations for batch gradient descent
    :param learning_rate:
    :return: optimized parameters, list of cost functions after each iteration
    """
    parameters = initialize_parameters(layer_dims)
    costs = np.zeros((iterations,1))
    for i in range(iterations):
        AL, forward_cache = forward_propagation(X, parameters)
        costs[i,0] = cost_function(AL, Y)
        parameters = update_parameters(forward_cache, parameters, AL, X, Y, learning_rate)

    return parameters, costs

