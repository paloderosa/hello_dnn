# Notes on forward and backward propagation in deep neural networks

The goals of these notes are twofold:

- expressing mathematically the procedures of forward and backward propagation;

- getting familiar with the notation employed in a full vectorized form of these operations, over features, hidden units, output units and training examples.

To the formality oriented, as myself, it is evident that forward and backward propagation are some complicated forms of function composition and chain rule application, respectively. However, these are not arbitrary instances of these mathematical rules. There is an underlying diagrammatic representation that defines the neural network and which we should take advantage of, but also that limits the class of functions expressible as the result of forward propagation. The generic architecture of a deep neural network is shown in the following figure. We count the total number of layers including the output layer but excluding the input layer. Therefore a neural network with L layers consists of one input layer, followed by L-1 hidden layers, followed by an output layer.

![general nn architecture](images/nn_diagram-1.png)

## Forward propagation

Let us consider the simplest case of a deep neural network, that with a single hidden layer and a single output unit, appropiate for a binary classification problem (see next image). At the moment, we are going to consider only one training example <img src="/tex/414cd160dadc48f33f87cff6680fb9fc.svg?invert_in_darkmode&sanitize=true" align=middle width=38.71763939999999pt height=24.65753399999998pt/> with <img src="/tex/b0ea07dc5c00127344a1cad40467b8de.svg?invert_in_darkmode&sanitize=true" align=middle width=9.97711604999999pt height=14.611878600000017pt/> arranged as a column vector with <img src="/tex/fe47ed6d5dbe8fdc90d1b93fc5552710.svg?invert_in_darkmode&sanitize=true" align=middle width=23.86239404999999pt height=29.190975000000005pt/> entries and <img src="/tex/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode&sanitize=true" align=middle width=8.649225749999989pt height=14.15524440000002pt/> a single classification output, as

<p align="center"><img src="/tex/39d8505bac17ee0e778f1fb0fe9aa48d.svg?invert_in_darkmode&sanitize=true" align=middle width=246.62472119999998pt height=69.04177335pt/></p>


![NN with a single hidden layer](images/nn_diagram_2_layers-1.png)



The first and only hidden layer has <img src="/tex/49ea3fbf9d60a6ca507c4688ac08afd1.svg?invert_in_darkmode&sanitize=true" align=middle width=23.86239404999999pt height=29.190975000000005pt/> units, which are activated by a function <img src="/tex/f98fd4e3a112fcef4eededf552a72a56.svg?invert_in_darkmode&sanitize=true" align=middle width=24.28087199999999pt height=29.190975000000005pt/> (tanh, relu or sigmoid, for example) and the output unit is activated by a function <img src="/tex/4d0a7bb512ff629072ec537959e43499.svg?invert_in_darkmode&sanitize=true" align=middle width=24.28087199999999pt height=29.190975000000005pt/>. The diagrammatic picture is the following. The circles in the input layer, also called the input units, represent each a feature <img src="/tex/77769095d6013b2ab9831e09876632d6.svg?invert_in_darkmode&sanitize=true" align=middle width=59.47009364999999pt height=34.337843099999986pt/>, so we have in total <img src="/tex/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode&sanitize=true" align=middle width=9.86687624999999pt height=14.15524440000002pt/> units. The following column of circles represent the units in the hidden layer.

![forward propagation in a single unit](images/nn_single_unit-1.png)

Consider a single unit <img src="/tex/36b5afebdba34564d884d347484ac0c7.svg?invert_in_darkmode&sanitize=true" align=middle width=7.710416999999989pt height=21.68300969999999pt/> in the hidden layer. This receives as input the units from the preceding layer and outputs a real function <img src="/tex/5288a793705077c312099be0c11f4aa1.svg?invert_in_darkmode&sanitize=true" align=middle width=22.684671899999987pt height=34.337843099999986pt/>, so that
<p align="center"><img src="/tex/595d748ee7a7903389989c7d6baa1f54.svg?invert_in_darkmode&sanitize=true" align=middle width=113.07082544999999pt height=24.7319028pt/></p>
which is produced by first computing
<p align="center"><img src="/tex/02e4f62893f8fe45ce04293571235212.svg?invert_in_darkmode&sanitize=true" align=middle width=174.24855689999998pt height=37.03214955pt/></p>
followed by the activation function \chi^{[i]}, so that
<p align="center"><img src="/tex/7fb048dc7b59dcc520e30f51706c009b.svg?invert_in_darkmode&sanitize=true" align=middle width=231.17958104999997pt height=49.315569599999996pt/></p>
where <img src="/tex/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode&sanitize=true" align=middle width=17.80826024999999pt height=22.465723500000017pt/> is a <img src="/tex/95b3ea6a2e2e425bcfc0cbe16017052a.svg?invert_in_darkmode&sanitize=true" align=middle width=68.63788304999999pt height=29.190975000000005pt/> matrix and <img src="/tex/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/> is a <img src="/tex/566d8948b5c85ac3979c2cee883f7278.svg?invert_in_darkmode&sanitize=true" align=middle width=52.99469834999999pt height=29.190975000000005pt/> matrix. And this happens for each unit in the hidden layer. Finally, the output unit performs the same action, takes as input the activation units from its preceding layer and outputs
<p align="center"><img src="/tex/9fe515b390dfee53fc7a44abf437aca6.svg?invert_in_darkmode&sanitize=true" align=middle width=231.17958104999997pt height=49.315569599999996pt/></p>
where now <img src="/tex/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/> is a <img src="/tex/6d02e53c2501c656ec5097d0439ff167.svg?invert_in_darkmode&sanitize=true" align=middle width=52.17279539999999pt height=29.190975000000005pt/> matrix and <img src="/tex/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/> is a <img src="/tex/b389d2f27360b3f4b0912983c08db155.svg?invert_in_darkmode&sanitize=true" align=middle width=36.52961069999999pt height=21.18721440000001pt/> matrix. The output <img src="/tex/4e8a5b79dbbfcb5883cc73a399461285.svg?invert_in_darkmode&sanitize=true" align=middle width=22.684671899999987pt height=29.190975000000005pt/> can be seen as <img src="/tex/f407d95f1cf595530728dba323fccc13.svg?invert_in_darkmode&sanitize=true" align=middle width=234.93545955000002pt height=29.190975000000005pt/>. This can be considered as a function <img src="/tex/8553dad814b1168c79f7a9e993549b48.svg?invert_in_darkmode&sanitize=true" align=middle width=71.2996053pt height=34.18097429999998pt/> with parameters <img src="/tex/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/>, <img src="/tex/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/> when, which varied, generate an entire subclass of functions in function space.

## Optimization task

Let us define the loss function
<p align="center"><img src="/tex/680a99a09a78a5d27885dac330331fa5.svg?invert_in_darkmode&sanitize=true" align=middle width=332.46212505pt height=29.58934275pt/></p>
This function penalizes that <img src="/tex/4e8a5b79dbbfcb5883cc73a399461285.svg?invert_in_darkmode&sanitize=true" align=middle width=22.684671899999987pt height=29.190975000000005pt/> differs from <img src="/tex/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode&sanitize=true" align=middle width=8.649225749999989pt height=14.15524440000002pt/>. For example, if the true label is 1 and the output unit gives 0, the loss function returns infinity, while if the output unit produces a 1, the loss is 0. We fix <img src="/tex/b0ea07dc5c00127344a1cad40467b8de.svg?invert_in_darkmode&sanitize=true" align=middle width=9.97711604999999pt height=14.611878600000017pt/> and consider the optimization task of finding the parameters <img src="/tex/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/d9c4a79964b086ebf56774d22db319cd.svg?invert_in_darkmode&sanitize=true" align=middle width=10.77628199999999pt height=29.190975000000005pt/>1<img src="/tex/9717eb07bba4963ba36bc0fe02be3033.svg?invert_in_darkmode&sanitize=true" align=middle width=4.5662248499999905pt height=24.65753399999998pt/>, <img src="/tex/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/> that minimize the loss function. This is achieved by starting from a random point in the space of parameters and following the path of greatest descent until some minimum is found. Since the surface over parameter space corresponding to the loss function is not convex, this minimum might be a local minimum. Random initialization allows to arrive at the global minimum. The path of greatest descent parameterized by <img src="/tex/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode&sanitize=true" align=middle width=7.7054801999999905pt height=14.15524440000002pt/> is defined by
<p align="center"><img src="/tex/9801bb215828c72cf9a7611ab940d34b.svg?invert_in_darkmode&sanitize=true" align=middle width=139.32328905pt height=44.1686784pt/></p>
where <img src="/tex/f166369f3ef0a7ff052f1e9bbf57d2e2.svg?invert_in_darkmode&sanitize=true" align=middle width=12.36779114999999pt height=22.831056599999986pt/> is some parameter and <img src="/tex/14177a8000e0fdbecbb22d4fe5f51696.svg?invert_in_darkmode&sanitize=true" align=middle width=18.06134714999999pt height=22.831056599999986pt/> is the metric in these coordinates. Therefore, it is important that we obtain an expression for the derivates of <img src="/tex/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode&sanitize=true" align=middle width=11.337943649999989pt height=22.465723500000017pt/> with respect to parameters.

## Backward propagation
\noindent Let us first consider the derivatives of <img src="/tex/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode&sanitize=true" align=middle width=11.337943649999989pt height=22.465723500000017pt/> with respect to parameters in the second layer, <img src="/tex/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/>. Typically the output unit is activated with the sigmoid function <img src="/tex/6f25c3c1b38a09912f9c013c93763221.svg?invert_in_darkmode&sanitize=true" align=middle width=31.135933949999988pt height=24.65753399999998pt/>, which satisfies <img src="/tex/5fa9a3a4659bcefa05762a41b55e7a02.svg?invert_in_darkmode&sanitize=true" align=middle width=161.0331426pt height=24.7161288pt/>. We have then
<p align="center"><img src="/tex/bc4faf78f2eb45a78b87aad5b06da1c0.svg?invert_in_darkmode&sanitize=true" align=middle width=273.74993415pt height=133.75982399999998pt/></p>
That was easy. Let us now compute the derivatives of the loss function with respect to farther parameters, those in the first layer, <img src="/tex/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/>, <img src="/tex/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode&sanitize=true" align=middle width=21.05031389999999pt height=29.190975000000005pt/>. In this case,
<p align="center"><img src="/tex/73430707b3f43d68a4844f9b6c1e4bf0.svg?invert_in_darkmode&sanitize=true" align=middle width=441.89071905000003pt height=179.60712494999999pt/></p>

In order to formulate a vectorization over features and units of the previous expressions, we consider the following. First, the series of derivatives along a set of parameters will be contained in an object of the same form as those parameters. For example, since <img src="/tex/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=29.190975000000005pt/> is a <img src="/tex/13ee291f362295925e67ad5b4edc7cb5.svg?invert_in_darkmode&sanitize=true" align=middle width=57.56092154999999pt height=29.190975000000005pt/> matrix, then <img src="/tex/cba58c126ed1a298e2f0c8b8878752a9.svg?invert_in_darkmode&sanitize=true" align=middle width=40.04594879999999pt height=22.465723500000017pt/> should be of the same form. Additionally, we define an operation <img src="/tex/7c74eeb32158ff7c4f67d191b95450fb.svg?invert_in_darkmode&sanitize=true" align=middle width=8.219209349999991pt height=15.296829900000011pt/> ocurring between matrices of the same and denoting element-wise multiplication. Then a vectorization is given by
<p align="center"><img src="/tex/411f82af989ba0befb6d11ab9b9875d5.svg?invert_in_darkmode&sanitize=true" align=middle width=194.69291654999998pt height=85.79216085pt/></p>
as well as
<p align="center"><img src="/tex/37e06d8ba31095312614b5b764ee42d2.svg?invert_in_darkmode&sanitize=true" align=middle width=351.37572029999995pt height=124.12687155pt/></p>



![backward propagation](images/nn_2_layers_backprop-1.png)

Let us express the previous results from a diagrammatic perspective, considering the figure above. Let us sit at the output unit, where the variation in the loss due to a variation in the linear output <img src="/tex/ab417fa2ae91dbc9948a205e0f494598.svg?invert_in_darkmode&sanitize=true" align=middle width=22.36313639999999pt height=29.190975000000005pt/> is <img src="/tex/2502ab54c4721e6c654d8af886a4ec8e.svg?invert_in_darkmode&sanitize=true" align=middle width=52.24699094999999pt height=29.190975000000005pt/>. Then we append that value to that unit's output. Then, going backward, we consider the line that joins the output unit with the <img src="/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode&sanitize=true" align=middle width=9.075367949999992pt height=22.831056599999986pt/>-th unit from the hidden layer. This line contributes with the weight <img src="/tex/7da65d40d4cc85c8194b6df386d162cc.svg?invert_in_darkmode&sanitize=true" align=middle width=31.80377474999999pt height=34.337843099999986pt/>, whose variation produces a variation in the loss. Then we only multiply the value from the left with the activation <img src="/tex/046e3a46e258c6388705ae4cb20f3850.svg?invert_in_darkmode&sanitize=true" align=middle width=22.684671899999987pt height=34.337843099999986pt/>, the same as for the bias unit activation (which equals one). As we go on, from the <img src="/tex/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode&sanitize=true" align=middle width=9.075367949999992pt height=22.831056599999986pt/>-th unit in the hidden layer to the <img src="/tex/36b5afebdba34564d884d347484ac0c7.svg?invert_in_darkmode&sanitize=true" align=middle width=7.710416999999989pt height=21.68300969999999pt/>-th unit in the input layer, we multiply first by <img src="/tex/4d4b0b16eefe480278356dceb421392a.svg?invert_in_darkmode&sanitize=true" align=middle width=132.73067444999998pt height=34.337843099999986pt/> and second by the activation <img src="/tex/b9a2205195e04327b045e6ff8cdba7ad.svg?invert_in_darkmode&sanitize=true" align=middle width=22.23661274999999pt height=14.15524440000002pt/> or by one for the bias unit.

## Vectorizing over <img src="/tex/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode&sanitize=true" align=middle width=14.433101099999991pt height=14.15524440000002pt/> examples
\noindent We now include in the <img src="/tex/32a7ff0eae3870fc401ce87fb7cd4460.svg?invert_in_darkmode&sanitize=true" align=middle width=59.20859009999999pt height=29.190975000000005pt/> matrix <img src="/tex/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode&sanitize=true" align=middle width=14.908688849999992pt height=22.465723500000017pt/> all the training examples with each training <img src="/tex/552d5221d232d5be3aa730d48acf7633.svg?invert_in_darkmode&sanitize=true" align=middle width=17.10749864999999pt height=21.839370299999988pt/> example as column vector. We also consider <img src="/tex/78eec7167c4a109dba4608e37317d844.svg?invert_in_darkmode&sanitize=true" align=middle width=59.20859009999999pt height=29.190975000000005pt/> vector <img src="/tex/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode&sanitize=true" align=middle width=13.19638649999999pt height=22.465723500000017pt/> as consisting of all the training results. Then we have
<p align="center"><img src="/tex/63820c81f75e99102bd1ca4b624b37d3.svg?invert_in_darkmode&sanitize=true" align=middle width=625.3167887999999pt height=114.51356234999999pt/></p>
We consider <img src="/tex/450d3e7588f21024213f0deca463e4a8.svg?invert_in_darkmode&sanitize=true" align=middle width=63.97253939999999pt height=29.190975000000005pt/>. Then \textbf{forward propagation} consists in the following series of steps:

- linear propagation to the first layer (yielding a <img src="/tex/c3ff9e3465a1a9ed6274c754938c22bf.svg?invert_in_darkmode&sanitize=true" align=middle width=247.23744374999995pt height=29.190975000000005pt/> matrix):
<p align="center"><img src="/tex/48927ae7e0b0309af492b8c4567a8d77.svg?invert_in_darkmode&sanitize=true" align=middle width=155.43385275pt height=18.6137556pt/></p>
- unit activation in the first layer:
<p align="center"><img src="/tex/303d0104a9aee66f488e6e4b37598d0f.svg?invert_in_darkmode&sanitize=true" align=middle width=131.06158395pt height=29.58934275pt/></p>
- linear propagation to the second layer (yielding a <img src="/tex/431b27262cbaa6331d0964cef10fe228.svg?invert_in_darkmode&sanitize=true" align=middle width=247.23744374999995pt height=29.190975000000005pt/> matrix):
<p align="center"><img src="/tex/9e27757b4092ebd7018aa1dfc25c3ac3.svg?invert_in_darkmode&sanitize=true" align=middle width=155.43385275pt height=18.6137556pt/></p>
- unit activation in the second layer:
<p align="center"><img src="/tex/9e43a17d5874eca17cc19d226a0340d1.svg?invert_in_darkmode&sanitize=true" align=middle width=115.94169015pt height=29.58934275pt/></p>

We define <img src="/tex/c38127c315bfa4cde3e353fda251eb75.svg?invert_in_darkmode&sanitize=true" align=middle width=46.754591399999995pt height=28.989011700000017pt/>  as a <img src="/tex/8769360a81bb4d3d8c7d04b73e4b07c5.svg?invert_in_darkmode&sanitize=true" align=middle width=53.751950999999984pt height=19.1781018pt/> matrix full of 1's, which can be programmed through broadcasting operations. We compute the \textbf{cost function} as the average of the individial losses over all examples as
<p align="center"><img src="/tex/9cd5188d3c07b557d124f4cc0698a88a.svg?invert_in_darkmode&sanitize=true" align=middle width=533.7305852999999pt height=32.990165999999995pt/></p>
Finally, \textbf{backward propagation} is performed through the following series of steps:
\begin{enumerate}
- output layer linear variation:
<p align="center"><img src="/tex/ef857d766f0bfd640c406c6a55ceee9d.svg?invert_in_darkmode&sanitize=true" align=middle width=179.06775149999999pt height=32.990165999999995pt/></p>
- weights and biases variations in the second layer:
<p align="center"><img src="/tex/fd8e7cead8fd844cf31020744dada576.svg?invert_in_darkmode&sanitize=true" align=middle width=205.7376651pt height=64.01096955pt/></p>
- hidden layer linear variation:
<p align="center"><img src="/tex/36b0af9b3459b767e62d70dc30023a50.svg?invert_in_darkmode&sanitize=true" align=middle width=302.22928394999997pt height=41.33280524999999pt/></p>
- weights and biases variations in the second layer:
<p align="center"><img src="/tex/16b2805ff8259d6660a906a884d8e0c4.svg?invert_in_darkmode&sanitize=true" align=middle width=205.7376651pt height=64.01096955pt/></p>



## Generalizing to arbitrary number of hidden layers
From the preceding discussion, it is straightforward to derive a generalization to a neural network with an arbitrary number of layers. Let us consider a neural network with the following architecture:

![dnn architecture](images/deep_network_architecture-1.png)



```
