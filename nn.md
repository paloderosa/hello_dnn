# Notes on forward and backward propagation in deep neural networks

The goals of these notes are twofold:

- expressing mathematically the procedures of forward and backward propagation;

- getting familiar with the notation employed in a full vectorized form of these operations, over features, hidden units, output units and training examples.

To the formality oriented, as myself, it is evident that forward and backward propagation are some complicated forms of function composition and chain rule application, respectively. However, these are not arbitrary instances of these mathematical rules. There is an underlying diagrammatic representation that defines the neural network and which we should take advantage of, but also that limits the class of functions expressible as the result of forward propagation. The generic architecture of a deep neural network is shown in the following figure. We count the total number of layers including the output layer but excluding the input layer. Therefore a neural network with L layers consists of one input layer, followed by L-1 hidden layers, followed by an output layer.

![general nn architecture](images/nn_diagram-1.png)

## Forward propagation

Let us consider the simplest case of a deep neural network, that with a single hidden layer and a single output unit, appropiate for a binary classification problem (see next image). At the moment, we are going to consider only one training example <img alt="$(\mathbf{x},y)$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/414cd160dadc48f33f87cff6680fb9fc.svg?invert_in_darkmode" align="middle" width="38.717745pt" height="24.65759999999998pt"/> with <img alt="$\mathbf{x}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/b0ea07dc5c00127344a1cad40467b8de.svg?invert_in_darkmode" align="middle" width="9.977220000000004pt" height="14.61206999999998pt"/> arranged as a column vector with <img alt="$n^{[0]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/fe47ed6d5dbe8fdc90d1b93fc5552710.svg?invert_in_darkmode" align="middle" width="23.862465000000004pt" height="29.19113999999999pt"/> entries and <img alt="$y$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align="middle" width="8.649300000000004pt" height="14.155350000000013pt"/> a single classification output, as

<p align="center"><img alt="$$&#10;\mathbf{x} = &#10; \begin{pmatrix}&#10; x_1 \\&#10; \vdots \\&#10; x_n&#10; \end{pmatrix}\in\mathbb{R}^{n^{[0]}}, \qquad y\in\{0,1\}.&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/39d8505bac17ee0e778f1fb0fe9aa48d.svg?invert_in_darkmode" align="middle" width="246.6255pt" height="69.041775pt"/></p>


![NN with a single hidden layer](images/nn_diagram_2_layers-1.png)



The first and only hidden layer has <img alt="$n^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/49ea3fbf9d60a6ca507c4688ac08afd1.svg?invert_in_darkmode" align="middle" width="23.862465000000004pt" height="29.19113999999999pt"/> units, which are activated by a function <img alt="$\chi^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/f98fd4e3a112fcef4eededf552a72a56.svg?invert_in_darkmode" align="middle" width="24.280905pt" height="29.19113999999999pt"/> (tanh, relu or sigmoid, for example) and the output unit is activated by a function <img alt="$\chi^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/4d0a7bb512ff629072ec537959e43499.svg?invert_in_darkmode" align="middle" width="24.280905pt" height="29.19113999999999pt"/>. The diagrammatic picture is the following. The circles in the input layer, also called the input units, represent each a feature <img alt="$a^{[0]}_i = x_i$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/77769095d6013b2ab9831e09876632d6.svg?invert_in_darkmode" align="middle" width="59.470125pt" height="34.33782pt"/>, so we have in total <img alt="$n$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align="middle" width="9.867000000000003pt" height="14.155350000000013pt"/> units. The following column of circles represent the units in the hidden layer.

![forward propagation in a single unit](images/nn_single_unit-1.png)

Consider a single unit <img alt="$j$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/36b5afebdba34564d884d347484ac0c7.svg?invert_in_darkmode" align="middle" width="7.710483000000004pt" height="21.683310000000006pt"/> in the hidden layer. This receives as input the units from the preceding layer and outputs a real function <img alt="$a^{[1]}_j$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/5288a793705077c312099be0c11f4aa1.svg?invert_in_darkmode" align="middle" width="22.684695000000005pt" height="34.33782pt"/>, so that
<p align="center"><img alt="$$&#10;a^{[1]}_j : \mathbb{R}^{n^{[0]}}\rightarrow\mathbb{R},&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/595d748ee7a7903389989c7d6baa1f54.svg?invert_in_darkmode" align="middle" width="113.07087pt" height="24.73185pt"/></p>
which is produced by first computing
<p align="center"><img alt="$$&#10;z_j^{[1]} = \sum_k W_{jk}^{[1]}a^{[0]}_k + b_j^{[1]},&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/02e4f62893f8fe45ce04293571235212.svg?invert_in_darkmode" align="middle" width="174.24824999999998pt" height="37.032104999999994pt"/></p>
followed by the activation function \chi^{[i]}, so that
<p align="center"><img alt="$$&#10;a_j^{[1]} = \chi^{[1]}\left(\sum_k W_{jk}^{[1]}a^{[0]}_k + b_j^{[1]}\right),&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/7fb048dc7b59dcc520e30f51706c009b.svg?invert_in_darkmode" align="middle" width="231.17985000000002pt" height="49.31553pt"/></p>
where <img alt="$W$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align="middle" width="17.808285000000005pt" height="22.46574pt"/> is a <img alt="$n^{[1]}\times n^{[0]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/95b3ea6a2e2e425bcfc0cbe16017052a.svg?invert_in_darkmode" align="middle" width="68.63802pt" height="29.19113999999999pt"/> matrix and <img alt="$b^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/> is a <img alt="$n^{[1]}\times 1$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/566d8948b5c85ac3979c2cee883f7278.svg?invert_in_darkmode" align="middle" width="52.9947pt" height="29.19113999999999pt"/> matrix. And this happens for each unit in the hidden layer. Finally, the output unit performs the same action, takes as input the activation units from its preceding layer and outputs
<p align="center"><img alt="$$&#10;a^{[2]} = \chi^{[2]}\left(\sum_k W_{1k}^{[2]}a^{[1]}_k + b^{[2]}\right),&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9fe515b390dfee53fc7a44abf437aca6.svg?invert_in_darkmode" align="middle" width="231.17985000000002pt" height="49.31553pt"/></p>
where now <img alt="$W^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/> is a <img alt="$1\times n^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/6d02e53c2501c656ec5097d0439ff167.svg?invert_in_darkmode" align="middle" width="52.172835pt" height="29.19113999999999pt"/> matrix and <img alt="$b^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/> is a <img alt="$1\times 1$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/b389d2f27360b3f4b0912983c08db155.svg?invert_in_darkmode" align="middle" width="36.52968pt" height="21.18732pt"/> matrix. The output <img alt="$a^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/4e8a5b79dbbfcb5883cc73a399461285.svg?invert_in_darkmode" align="middle" width="22.684695000000005pt" height="29.19113999999999pt"/> can be seen as <img alt="$a^{[2]} = a^{[2]}\left(\mathbf{x};W^{[1]},W^{[2]},b^{[1]},b^{[2]}\right)$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/f407d95f1cf595530728dba323fccc13.svg?invert_in_darkmode" align="middle" width="234.934755pt" height="29.19113999999999pt"/>. This can be considered as a function <img alt="$\mathbb{R}^{n^{[0]}}\rightarrow\mathbb{R}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/8553dad814b1168c79f7a9e993549b48.svg?invert_in_darkmode" align="middle" width="71.299635pt" height="34.18107pt"/> with parameters <img alt="$W^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$W^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$b^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/>, <img alt="$b^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/> when, which varied, generate an entire subclass of functions in function space.

## Optimization task

Let us define the loss function
<p align="center"><img alt="$$&#10;\mathcal{L}(a^{[2]},y) = -y\log a^{[2]} - (1-y)\log\left(1-a^{[2]}\right).&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/680a99a09a78a5d27885dac330331fa5.svg?invert_in_darkmode" align="middle" width="332.4618pt" height="29.589285pt"/></p>
This function penalizes that <img alt="$a^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/4e8a5b79dbbfcb5883cc73a399461285.svg?invert_in_darkmode" align="middle" width="22.684695000000005pt" height="29.19113999999999pt"/> differs from <img alt="$y$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align="middle" width="8.649300000000004pt" height="14.155350000000013pt"/>. For example, if the true label is 1 and the output unit gives 0, the loss function returns infinity, while if the output unit produces a 1, the loss is 0. We fix <img alt="$\mathbf{x}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/b0ea07dc5c00127344a1cad40467b8de.svg?invert_in_darkmode" align="middle" width="9.977220000000004pt" height="14.61206999999998pt"/> and consider the optimization task of finding the parameters <img alt="$W^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$W^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$b^{[$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/d9c4a79964b086ebf56774d22db319cd.svg?invert_in_darkmode" align="middle" width="10.776315000000002pt" height="29.19113999999999pt"/>1<img alt="$]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9717eb07bba4963ba36bc0fe02be3033.svg?invert_in_darkmode" align="middle" width="4.566292500000005pt" height="24.65759999999998pt"/>, <img alt="$b^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/> that minimize the loss function. This is achieved by starting from a random point in the space of parameters and following the path of greatest descent until some minimum is found. Since the surface over parameter space corresponding to the loss function is not convex, this minimum might be a local minimum. Random initialization allows to arrive at the global minimum. The path of greatest descent parameterized by <img alt="$s$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align="middle" width="7.705549500000004pt" height="14.155350000000013pt"/> is defined by
<p align="center"><img alt="$$&#10; \frac{d\theta_i}{ds} = - \sum_j \delta_{ij}\frac{\partial\mathcal{L}}{\partial\theta_j},&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9801bb215828c72cf9a7611ab940d34b.svg?invert_in_darkmode" align="middle" width="139.32336pt" height="44.168684999999996pt"/></p>
where <img alt="$\theta_i$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/f166369f3ef0a7ff052f1e9bbf57d2e2.svg?invert_in_darkmode" align="middle" width="12.367905000000004pt" height="22.831379999999992pt"/> is some parameter and <img alt="$\delta_{ij}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/14177a8000e0fdbecbb22d4fe5f51696.svg?invert_in_darkmode" align="middle" width="18.061395000000005pt" height="22.831379999999992pt"/> is the metric in these coordinates. Therefore, it is important that we obtain an expression for the derivates of <img alt="$\mathcal{L}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode" align="middle" width="11.337975000000004pt" height="22.46574pt"/> with respect to parameters.

## Backward propagation
\noindent Let us first consider the derivatives of <img alt="$\mathcal{L}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode" align="middle" width="11.337975000000004pt" height="22.46574pt"/> with respect to parameters in the second layer, <img alt="$W^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$b^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/d0d99e9a5838d2ffd9f9a2d65cea533e.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/>. Typically the output unit is activated with the sigmoid function <img alt="$\sigma(z)$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/6f25c3c1b38a09912f9c013c93763221.svg?invert_in_darkmode" align="middle" width="31.135995000000005pt" height="24.65759999999998pt"/>, which satisfies <img alt="$\sigma'(z) = \sigma(z)(1-\sigma(z))$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/5fa9a3a4659bcefa05762a41b55e7a02.svg?invert_in_darkmode" align="middle" width="161.03290499999997pt" height="24.716340000000006pt"/>. We have then
<p align="center"><img alt="$$&#10; \begin{aligned}&#10; &#9;\frac{\partial\mathcal{L}}{\partial z^{[2]}} &amp; = \frac{\partial\mathcal{L}}{\partial a^{[2]}}\frac{d a^{[2]}}{d z^{[2]}} = a^{[2]}-y,\\&#10;&#9;\frac{\partial\mathcal{L}}{\partial W_{1k}^{[2]}} &amp; = \frac{\partial\mathcal{L}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial W_{1k}^{[2]}} = \left(a^{[2]}-y\right)a^{[1]}_k, \\&#10;&#9;\frac{\partial\mathcal{L}}{\partial b^{[2]}} &amp; = \frac{\partial\mathcal{L}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial b^{[2]}} = a^{[2]}-y.&#10; \end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/bc4faf78f2eb45a78b87aad5b06da1c0.svg?invert_in_darkmode" align="middle" width="273.74985pt" height="133.75988999999998pt"/></p>
That was easy. Let us now compute the derivatives of the loss function with respect to farther parameters, those in the first layer, <img alt="$W^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/e99a4e1d357aa81fae3ff5e34777a7d3.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/>, <img alt="$b^{[1]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/39c7d8201e2cadb69c40aa59b2b65d48.svg?invert_in_darkmode" align="middle" width="21.050370000000004pt" height="29.19113999999999pt"/>. In this case,
<p align="center"><img alt="$$&#10;\begin{aligned}&#10;  \frac{\partial\mathcal{L}}{\partial z^{[1]}_j} &amp; = \sum_l\frac{\partial\mathcal{L}}{\partial a^{[2]}}\frac{d a^{[2]}}{d z^{[2]}}\frac{\partial z^{[2]}}{\partial a^{[1]}_l}\frac{d a^{[1]}_l}{d z^{[1]}_j} = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j},\\ &#10;  \frac{\partial\mathcal{L}}{\partial W^{[1]}_{jk}} &amp; = \sum_{m}\frac{\partial\mathcal{L}}{\partial z^{[1]}_m}\frac{\partial z^{[1]}_m}{\partial W_{jk}^{[1]}}  = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j}a_k^{[0]}, \\&#10;  \frac{\partial\mathcal{L}}{\partial b^{[1]}_{j}} &amp; = \sum_{m}\frac{\partial\mathcal{L}}{\partial z^{[1]}_m}\frac{\partial z^{[1]}_m}{\partial b_{j}^{[1]}}  = \frac{\partial\mathcal{L}}{\partial z^{[2]}} W^{[2]}_{1j}\frac{d\chi^{[1]}(z)}{dz}{\Bigg\arrowvert}_{z=z^{[1]}_j}.&#10;\end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/73430707b3f43d68a4844f9b6c1e4bf0.svg?invert_in_darkmode" align="middle" width="441.89144999999996pt" height="179.60744999999997pt"/></p>

In order to formulate a vectorization over features and units of the previous expressions, we consider the following. First, the series of derivatives along a set of parameters will be contained in an object of the same form as those parameters. For example, since <img alt="$W^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9d38689287a7b126924bced5db5791a0.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="29.19113999999999pt"/> is a <img alt="$1\times n^{[1]}]$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/13ee291f362295925e67ad5b4edc7cb5.svg?invert_in_darkmode" align="middle" width="57.56091000000001pt" height="29.19113999999999pt"/> matrix, then <img alt="$\nabla_{W^{[2]}}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/cba58c126ed1a298e2f0c8b8878752a9.svg?invert_in_darkmode" align="middle" width="40.045995000000005pt" height="22.46574pt"/> should be of the same form. Additionally, we define an operation <img alt="$*$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/7c74eeb32158ff7c4f67d191b95450fb.svg?invert_in_darkmode" align="middle" width="8.219277000000005pt" height="15.297149999999977pt"/> ocurring between matrices of the same and denoting element-wise multiplication. Then a vectorization is given by
<p align="center"><img alt="$$&#10; \begin{aligned}&#10;  \nabla_{z^{[2]}}\mathcal{L} &amp; = a^{[2]}-y, \\&#10;  \nabla_{W^{[2]}}\mathcal{L} &amp; = \left(\nabla_{z^{[2]}\mathcal{L}}\right)\left(a^{[1]}\right)^T, \\&#10;  \nabla_{b^{[2]}}\mathcal{L} &amp; = \left(\nabla_{z^{[2]}}\mathcal{L}\right),&#10; \end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/411f82af989ba0befb6d11ab9b9875d5.svg?invert_in_darkmode" align="middle" width="194.69339999999997pt" height="85.79207999999998pt"/></p>
as well as
<p align="center"><img alt="$$&#10;\begin{aligned}&#10;\nabla_{z^{[1]}}\mathcal{L} &amp;  = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz},\\ &#10;\nabla_{W^{[1]}}\mathcal{L} &amp; = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz}\left(a^{[0]}\right)^T, \\&#10;\nabla_{b^{[1]}}\mathcal{L} &amp; = \left(\nabla_{z^{[2]}}\mathcal{L}\right) \left(W^{[2]}\right)^T * \frac{d\chi^{[1]}(z^{[1]})}{dz}.&#10;\end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/37e06d8ba31095312614b5b764ee42d2.svg?invert_in_darkmode" align="middle" width="351.37575pt" height="124.12685999999998pt"/></p>



![backward propagation](images/nn_2_layers_backprop-1.png)

Let us express the previous results from a diagrammatic perspective, considering the figure above. Let us sit at the output unit, where the variation in the loss due to a variation in the linear output <img alt="$z^{[2]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/ab417fa2ae91dbc9948a205e0f494598.svg?invert_in_darkmode" align="middle" width="22.363275000000005pt" height="29.19113999999999pt"/> is <img alt="$a^{[2]} - y$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/2502ab54c4721e6c654d8af886a4ec8e.svg?invert_in_darkmode" align="middle" width="52.247085000000006pt" height="29.19113999999999pt"/>. Then we append that value to that unit's output. Then, going backward, we consider the line that joins the output unit with the <img alt="$k$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align="middle" width="9.075495000000004pt" height="22.831379999999992pt"/>-th unit from the hidden layer. This line contributes with the weight <img alt="$W^{[2]}_k$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/7da65d40d4cc85c8194b6df386d162cc.svg?invert_in_darkmode" align="middle" width="31.803915pt" height="34.33782pt"/>, whose variation produces a variation in the loss. Then we only multiply the value from the left with the activation <img alt="$a^{[1]}_k$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/046e3a46e258c6388705ae4cb20f3850.svg?invert_in_darkmode" align="middle" width="22.684695000000005pt" height="34.33782pt"/>, the same as for the bias unit activation (which equals one). As we go on, from the <img alt="$k$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align="middle" width="9.075495000000004pt" height="22.831379999999992pt"/>-th unit in the hidden layer to the <img alt="$j$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/36b5afebdba34564d884d347484ac0c7.svg?invert_in_darkmode" align="middle" width="7.710483000000004pt" height="21.683310000000006pt"/>-th unit in the input layer, we multiply first by <img alt="$W^{[2]}_{ik} (d\chi^{[1]}(z_k)/dz)$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/4d4b0b16eefe480278356dceb421392a.svg?invert_in_darkmode" align="middle" width="132.730785pt" height="34.33782pt"/> and second by the activation <img alt="$a_{[j]}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/b9a2205195e04327b045e6ff8cdba7ad.svg?invert_in_darkmode" align="middle" width="22.236720000000005pt" height="14.155350000000013pt"/> or by one for the bias unit.

## Vectorizing over <img alt="$m$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align="middle" width="14.433210000000003pt" height="14.155350000000013pt"/> examples
\noindent We now include in the <img alt="$n^{[0]}\times m$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/32a7ff0eae3870fc401ce87fb7cd4460.svg?invert_in_darkmode" align="middle" width="59.2086pt" height="29.19113999999999pt"/> matrix <img alt="$X$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode" align="middle" width="14.908740000000003pt" height="22.46574pt"/> all the training examples with each training <img alt="$\mathbf{x}^a$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/552d5221d232d5be3aa730d48acf7633.svg?invert_in_darkmode" align="middle" width="17.107530000000004pt" height="21.839399999999983pt"/> example as column vector. We also consider <img alt="$n^{[2]}\times m$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/78eec7167c4a109dba4608e37317d844.svg?invert_in_darkmode" align="middle" width="59.2086pt" height="29.19113999999999pt"/> vector <img alt="$Y$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode" align="middle" width="13.196370000000005pt" height="22.46574pt"/> as consisting of all the training results. Then we have
<p align="center"><img alt="$$&#10;X = \begin{pmatrix}&#10; x^1_1 &amp; x^2_1 &amp; \cdots &amp; x^{m-1}_1 &amp; x^m_1 \\&#10; x^1_2 &amp; x^2_2 &amp; \cdots &amp; x^{m-1}_2 &amp; x^m_2 \\&#10; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\&#10; x^1_{n^{[0]}-1} &amp; x^2_{n^{[0]}-1} &amp; \cdots &amp; x^{m-1}_{n^{[0]}-1} &amp; x^m_{n^{[0]}-1} \\&#10; x^1_{n^{[0]}} &amp; x^2_{n^{[0]}} &amp; \cdots &amp; x^{m-1}_{n^{[0]}} &amp; x^m_{n^{[0]}} \\&#10;\end{pmatrix}, \qquad &#10;Y = \begin{pmatrix}&#10;y^1 &amp; y^2 &amp; \cdots &amp; y^{m-1} &amp; y^m \\&#10;\end{pmatrix}.&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/63820c81f75e99102bd1ca4b624b37d3.svg?invert_in_darkmode" align="middle" width="625.317pt" height="114.51362999999998pt"/></p>
We consider <img alt="$A^{[0]} = X$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/450d3e7588f21024213f0deca463e4a8.svg?invert_in_darkmode" align="middle" width="63.972645pt" height="29.19113999999999pt"/>. Then \textbf{forward propagation} consists in the following series of steps:

- linear propagation to the first layer (yielding a <img alt="$(n^{[1]}\times n^{[0]})\cdot(n^{[0]}\times m) = n^{[1]}\times m$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/c3ff9e3465a1a9ed6274c754938c22bf.svg?invert_in_darkmode" align="middle" width="247.23715499999997pt" height="29.19113999999999pt"/> matrix):
<p align="center"><img alt="$$&#10;&#9;Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]};&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/48927ae7e0b0309af492b8c4567a8d77.svg?invert_in_darkmode" align="middle" width="155.43379499999998pt" height="18.613815pt"/></p>
- unit activation in the first layer:
<p align="center"><img alt="$$&#10;&#9; A^{[1]} = \chi^{[1]}\left(Z^{[1]}\right);&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/303d0104a9aee66f488e6e4b37598d0f.svg?invert_in_darkmode" align="middle" width="131.06164499999997pt" height="29.589285pt"/></p>
- linear propagation to the second layer (yielding a <img alt="$(n^{[2]}\times n^{[1]})\cdot(n^{[1]}\times m) = n^{[2]}\times m$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/431b27262cbaa6331d0964cef10fe228.svg?invert_in_darkmode" align="middle" width="247.23715499999997pt" height="29.19113999999999pt"/> matrix):
<p align="center"><img alt="$$&#10;&#9;Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]};&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9e27757b4092ebd7018aa1dfc25c3ac3.svg?invert_in_darkmode" align="middle" width="155.43379499999998pt" height="18.613815pt"/></p>
- unit activation in the second layer:
<p align="center"><img alt="$$&#10;&#9;A^{[2]} = \sigma\left(Z^{[2]}\right).&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9e43a17d5874eca17cc19d226a0340d1.svg?invert_in_darkmode" align="middle" width="115.941705pt" height="29.589285pt"/></p>

We define <img alt="$\dot{1}_{n_1\times n_2}$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/c38127c315bfa4cde3e353fda251eb75.svg?invert_in_darkmode" align="middle" width="46.75473000000001pt" height="28.98917999999999pt"/>  as a <img alt="$n_1\times n_2$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/8769360a81bb4d3d8c7d04b73e4b07c5.svg?invert_in_darkmode" align="middle" width="53.752050000000004pt" height="19.178279999999994pt"/> matrix full of 1's, which can be programmed through broadcasting operations. We compute the \textbf{cost function} as the average of the individial losses over all examples as
<p align="center"><img alt="$$&#10; \mathcal{L}\left(A^{[2]},Y\right) = -\frac{1}{m}\left[\left(\log A^{[2]}\right) Y^T + \left(\dot{1}_{n^{[2]}\times m}-\log A^{[2]}\right) \left(\dot{1}_{n^{[2]}\times m} - Y\right)^T\right].&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/9cd5188d3c07b557d124f4cc0698a88a.svg?invert_in_darkmode" align="middle" width="533.73045pt" height="32.9901pt"/></p>
Finally, \textbf{backward propagation} is performed through the following series of steps:
\begin{enumerate}
- output layer linear variation:
<p align="center"><img alt="$$&#10;&#9; \nabla_{Z^{[2]}} \mathcal{L} = \frac{1}{m}\left(A^{[2]} - Y\right);&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/ef857d766f0bfd640c406c6a55ceee9d.svg?invert_in_darkmode" align="middle" width="179.06789999999998pt" height="32.9901pt"/></p>
- weights and biases variations in the second layer:
<p align="center"><img alt="$$&#10;\begin{aligned}&#10;&#9; \nabla_{W^{[2]}}\mathcal{L} &amp; = \nabla_{Z^{[2]}} \mathcal{L} \cdot \left(A^{[1]}\right)^T, \\&#10;&#9; \nabla_{b^{[2]}}\mathcal{L} &amp; = \nabla_{Z^{[2]}} \mathcal{L} \cdot \left(\dot{1}_{1\times m}\right)^T;&#10;\end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/fd8e7cead8fd844cf31020744dada576.svg?invert_in_darkmode" align="middle" width="205.73684999999998pt" height="64.010925pt"/></p>
- hidden layer linear variation:
<p align="center"><img alt="$$&#10;&#9;\nabla_{Z^{[1]}} \mathcal{L} = \left[\left(W^{[2]}\right)^T\cdot\nabla_{Z^{[2]}} \mathcal{L}\right]*\frac{d\chi^{[1]}(Z^{[1]})}{dz}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/36b0af9b3459b767e62d70dc30023a50.svg?invert_in_darkmode" align="middle" width="302.22885pt" height="41.33283pt"/></p>
- weights and biases variations in the second layer:
<p align="center"><img alt="$$&#10;\begin{aligned}&#10;&#9;\nabla_{W^{[1]}}\mathcal{L} &amp; = \nabla_{Z^{[1]}} \mathcal{L} \cdot \left(A^{[0]}\right)^T, \\&#10;&#9;\nabla_{b^{[1]}}\mathcal{L} &amp; = \nabla_{Z^{[1]}} \mathcal{L} \cdot \left(\dot{1}_{1\times m}\right)^T;&#10;\end{aligned}&#10;$$" src="https://rawgit.com/paloderosa/my_dnn/None/svgs/16b2805ff8259d6660a906a884d8e0c4.svg?invert_in_darkmode" align="middle" width="205.73684999999998pt" height="64.010925pt"/></p>



## Generalizing to arbitrary number of hidden layers
From the preceding discussion, it is straightforward to derive a generalization to a neural network with an arbitrary number of layers. Let us consider a neural network with the following architecture:

![dnn architecture](images/deep_network_architecture-1.png)

### Forward propagation

```{r, eval = False, tidy = False}


```


### Backward propagation

```{r, eval = False, tidy = False}


```
